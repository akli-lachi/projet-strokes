# -*- coding: utf-8 -*-
"""Strokes

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAF-OvaDz1YgChInIwMDAGGCQXWDoCYl

1. Summary of the problem

2. Setup
2.1 Load libs

3. The data
3.1 Understanding data

4. Exploratory Data Analysis and vizalisation

5.  Data Cleaning
5.1 data missing 
5.2 Fill empty values
5.3 Impute empty values
5.4 Converting Categorical Features 

6.  Modeling and evaualtion
6.1 Train Test Split
6.2 Training and Predicting
6.3 Evaluation

# GOOGLE AUTHENTICATION
"""

from google.colab import drive
drive.mount('/content/drive')

"""***Summary of the problem***

**Goal**

The job is to predict if a person is going to suffer a stroke

We will try to predict a classification - Let's begin our understanding of the implementation of logistic regression in Python for classification.

**Import Libraries**

Let's import some libraries to get started!
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt;
import seaborn as sns

"""***The Data***"""

# importation du daset
df = pd.read_csv('https://assets-datascientest.s3-eu-west-1.amazonaws.com/de/total/strokes.csv', sep = ',', header = 0, index_col = 0)

#apperçu 
print(df.head())
df.tail()

#dimension et taille 
df.size
df.shape

#description et info 
df.info()
df.describe()
df.describe(include="O")

"""***Exploratory Data Analysis and vizalisation***

Missing Data 

Let's begin some exploratory data analysis! We'll start by checking out missing data!

We can use seaborn to create a simple heatmap to see where we are missing data!
"""

sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')

df.isnull().sum().sort_values(ascending=False)

sns.set_style('whitegrid')
sns.countplot(x='stroke',data=df,palette='RdBu_r')

sns.set_style('whitegrid')
sns.countplot(x='stroke',hue='gender',data=df,palette='RdBu_r')

sns.set_style('whitegrid')
sns.countplot(x='stroke',hue='heart_disease',data=df,palette='rainbow')

df['age'].hist(bins=30,color='darkred',alpha=0.7)

sns.countplot(x='hypertension',data=df)

"""***Data Cleaning***

visuation des valeurs manquantes en fonction de du genre
"""

plt.figure(figsize=(12, 7))
sns.boxplot(x='gender',y='bmi',data=df,palette='winter')

"""Fill empty values"""

# On remplace les NANs de la colonne 'bmi' par la moyenne de cette colonne arrondi à 1 décimale
df.bmi = df.bmi.fillna(round(df.bmi.mean(),1))

# On vérifie qu'il n'y a plus de lignes qui contiennent des NANs
lignes_na = df.isna().any(axis = 1)
print(lignes_na.sum(), "lignes contiennent des NANs. \n")

"""Converting Categorical Features"""

df.info()

# On regarde les valeurs de la colonne 'gender'
print(df['gender'].value_counts(),"\n")
# On récupère l'index de la ligne gender=other
indexOther = df[df['gender'] == 'Other'].index
# On supprime la ligne du dataFrame
df.drop(indexOther , inplace=True)
# On remplace les modalités 'Male' et 'Female' de la variable 'gender' par 0, 1
df.gender = df.gender.replace(['Male','Female'], [0,1])

# On regarde les valeurs de la colonne 'ever_married'
print(df['ever_married'].value_counts(),"\n")
# On remplace les modalités 'No' et 'Yes' de la variable 'ever_married' par 0 et 1
df.ever_married = df.ever_married.replace(['No','Yes'], [0,1])

# On regarde les valeurs de la colonne 'Residence_type'
print(df['Residence_type'].value_counts(),"\n")
# On remplace les modalités 'Rural' et 'Urban' de la variable 'Residence_type' par 0 et 1
df.Residence_type = df.Residence_type.replace(['Rural','Urban'], [0,1])
# On renomme la colonne 'Residence_type' en 'urban_residence'
df.rename(columns={'Residence_type': 'urban_residence'}, inplace=True)

# On regarde les valeurs de la colonne 'work_type'
print(df['work_type'].value_counts(),"\n")
# On supprime la colonne 'work_type'
df = df.drop(['work_type'], axis = 1)

# On regarde les valeurs de la colonne 'smoking_status'
print(df['smoking_status'].value_counts(),"\n")
# On remplace les modalités 'Unknown' et 'formerly smoked' de la variable 'smoking_status' par 0 et 1
df.smoking_status = df.smoking_status.replace(['Unknown','formerly smoked'], [0,1])
# On remplace les modalités 'never smoked' et 'smokes' de la variable 'smoking_status' par 0 et 1
df.smoking_status = df.smoking_status.replace(['never smoked','smokes'], [0,1])

df.head()

"""Great! Our data is ready for our model!

Building a Logistic Regression model

Let's start by splitting our data into a training set and test set

Train Test Split
"""

from sklearn.model_selection import train_test_split

pip install scikit-learn==0.24

#separation des variables explicatives dans un dataframe X et la variable cible dans y.
X = df.drop("stroke", axis = 1)
y = df.stroke

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.preprocessing import MinMaxScaler
from joblib import dump, load
scaler = MinMaxScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
dump(scaler, 'minmax_scaler.bin', compress=True)


# Normalisation Min-Max, de sorte que les variables soient toutes comprises dans l'intervalle fixe [0,1]
X = X.apply(lambda x : (x-x.min())/(x.max() - x.min()))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)


X_test.head()

# On affiche les dimensions des datasets après avoir appliqué la fonction 
print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)
print(type(X_train))

"""**LogisticRegression**"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report,confusion_matrix

model = LogisticRegression(max_iter=500)
model.fit(X_train,y_train)
predictions = model.predict(X_test)
X_test.head()



print("\nMatrice de Confusion lr:\n",  confusion_matrix(y_test,predictions))
print("\nL'accuracy est égale à :" + str(metrics.accuracy_score(y_test, predictions)))
print("La précision est égale à :" + str(metrics.precision_score(y_test, predictions)))
print("Le rappel est égal à :" + str(metrics.recall_score(y_test, predictions)))
print("Le score F1 est égal à :" + str(metrics.f1_score(y_test, predictions)))
print("Le MCC est égal à :" + str(metrics.matthews_corrcoef(y_test, predictions)))

"""**C-Support Vector Classification**"""

from sklearn.svm import SVC
from sklearn import metrics

svm = SVC(gamma ='scale', class_weight= "balanced")
svm.fit(X_train, y_train)    

preds = svm.predict(X_test)
pd.crosstab(y_test, preds)

print("\nL'accuracy est égale à :" + str(metrics.accuracy_score(y_test, preds)))
print("La précision est égale à :" + str(metrics.precision_score(y_test, preds)))
print("Le rappel est égal à :" + str(metrics.recall_score(y_test, preds)))
print("Le score F1 est égal à :" + str(metrics.f1_score(y_test, preds)))
print("Le MCC est égal à :" + str(metrics.matthews_corrcoef(y_test, preds)))

"""**Classification déséquilibrée**

Le nombre de cas positif (strokes=1) est trop faible dans le jeu de données. 
Nous allons augmenter le nombre d’observations de la classe minoritaire (oversampling). Nous privilégions le sur-échantillonnage car nous avons seulement quelques milliers de données.
"""

# Sur-échantillonnage ou oversampling
from imblearn.over_sampling  import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
rOs = RandomOverSampler()
X_train,y_train  = rOs.fit_resample(X_train, y_train)

print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)
print(type(X_train))

sns.set_style('whitegrid')
sns.countplot(x=y_train,palette='RdBu_r')

print("Total lignes:",len(y_train))
print("Lignes 1:",len(np.where(y_train==1)[0]))
print("Lignes 0:",len(np.where(y_train==0)[0]))

"""Training and Predicting

**Support Vector Classification**
"""

from sklearn.svm import SVC

sv_model = SVC(gamma ='scale', class_weight= "balanced")

sv_model.fit(X_train, y_train)    

preds = sv_model.predict(X_test)
pd.crosstab(y_test, preds)

"""**LogisticRegression**"""

from sklearn.pipeline import Pipeline
import joblib
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

#lr_model = Pipeline([('normalization', MinMaxScaler()),('classifier', LogisticRegression(max_iter=500))])

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)

lr_model = LogisticRegression(max_iter=500)
lr_model.fit(X_train,y_train)

predictions = lr_model.predict(X_test)

"""We can check precision,recall,f1-score using classification report!"""

from sklearn.metrics import classification_report,confusion_matrix

print("\nMatrice de Confusion lr:\n",  confusion_matrix(y_test,predictions))

print("\nRapports de classification du modèle lr :\n",classification_report(y_test,predictions))

print("\nL'accuracy est égale à :" + str(metrics.accuracy_score(y_test, predictions)))
print("La précision est égale à :" + str(metrics.precision_score(y_test, predictions)))
print("Le rappel est égal à :" + str(metrics.recall_score(y_test, predictions)))
print("Le score F1 est égal à :" + str(metrics.f1_score(y_test, predictions)))
print("Le MCC est égal à :" + str(metrics.matthews_corrcoef(y_test, predictions)))

"""***Decision Tree Classifiction***"""

from sklearn.tree import DecisionTreeClassifier

#dt_model = Pipeline([('normalization', MinMaxScaler()),('classifier', DecisionTreeClassifier())])
dt_model=DecisionTreeClassifier()
dt_model.fit(X_train,y_train)

dt_pred = dt_model.predict(X_test)

print("\nMatrice de Confusion dt:\n",  confusion_matrix(y_test,dt_pred))

print("\nRapports de classification du modèle dt :\n",classification_report(y_test,dt_pred))

print("\nL'accuracy est égale à :" + str(metrics.accuracy_score(y_test, dt_pred)))
print("La précision est égale à :" + str(metrics.precision_score(y_test, dt_pred)))
print("Le rappel est égal à :" + str(metrics.recall_score(y_test, dt_pred)))
print("Le score F1 est égal à :" + str(metrics.f1_score(y_test, dt_pred)))
print("Le MCC est égal à :" + str(metrics.matthews_corrcoef(y_test, dt_pred)))

"""***Random Forest Classification***"""

from sklearn.ensemble import RandomForestClassifier

#rf_model = Pipeline([('normalization', MinMaxScaler()),('classifier', RandomForestClassifier(n_estimators=500))])
rf_model= RandomForestClassifier(n_estimators=500)
rf_model.fit(X_train,y_train)

# Prédiction de la variable cible pour le jeu de données test
rf_pred = rf_model.predict(X_test)

print("\nMatrice de Confusion rf :\n",  confusion_matrix(y_test,rf_pred))

print("\nRapports de classification du modèle rf :\n",classification_report(y_test,rf_pred))

print("\nL'accuracy est égale à :" + str(metrics.accuracy_score(y_test, rf_pred)))
print("La précision est égale à :" + str(metrics.precision_score(y_test, rf_pred)))
print("Le rappel est égal à :" + str(metrics.recall_score(y_test, rf_pred)))
print("Le score F1 est égal à :" + str(metrics.f1_score(y_test, rf_pred)))
print("Le MCC est égal à :" + str(metrics.matthews_corrcoef(y_test, rf_pred)))

"""***XGBoosts Classifier***"""

!pip install xgboost

from xgboost import XGBClassifier
#xg_model = Pipeline([('normalization', MinMaxScaler()),('classifier', XGBClassifier(n_estimators=1000))])
xg_model = XGBClassifier(n_estimators=1000)
xg_model.fit(X_train,y_train)

# Prédiction de la variable cible pour le jeu de données test
xg_pred = xg_model.predict(X_test.values)

# Calcul et affichage de la matrice de confusion
print("\nMatrice de Confusion xg:\n",  confusion_matrix(y_test,xg_pred))

print("\nRapports de classification du modèle xg :\n",classification_report(y_test,xg_pred))

print("\nL'accuracy est égale à :" + str(metrics.accuracy_score(y_test, xg_pred)))
print("La précision est égale à :" + str(metrics.precision_score(y_test, xg_pred)))
print("Le rappel est égal à :" + str(metrics.recall_score(y_test, xg_pred)))
print("Le score F1 est égal à :" + str(metrics.f1_score(y_test, xg_pred)))
print("Le MCC est égal à :" + str(metrics.matthews_corrcoef(y_test, xg_pred)))

"""**K-Nearest Neighbors**"""

from sklearn.neighbors import KNeighborsClassifier

#kn_model = Pipeline([('normalization', MinMaxScaler()),('classifier', KNeighborsClassifier(n_neighbors = 6))])
kn_model = KNeighborsClassifier(n_neighbors = 6)    
kn_model.fit(X_train, y_train)

# Prédiction de la variable cible pour le jeu de données test. Ces prédictions sont stockées dans y_pred_test_knn 
y_pred_test_knn  = kn_model.predict(X_test)
print("\nK-Nearest Neighbors, y_pred_test_knn:", y_pred_test_knn[:10])

# Calcul et affichage de la matrice de confusion
matrice_confusion = confusion_matrix(y_test, y_pred_test_knn)
print("\nMatrice de Confusion knn:\n",  matrice_confusion)
print("\nRapports de classification du modèle knn :\n", classification_report(y_test, y_pred_test_knn))

"""**Export du modèle**"""

from joblib import dump, load
dump(lr_model, 'lr_model.joblib') 
dump(dt_model, 'dt_model.joblib') 
dump(kn_model, 'kn_model.joblib') 
dump(rf_model, 'rf_model.joblib') 
dump(xg_model, 'xg_model.joblib')